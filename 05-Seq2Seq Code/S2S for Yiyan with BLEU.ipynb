{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba  # 使用jieba进行中文分词\n",
    "import re  # 使用正则表达式对英文句子进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取句子\n",
    "with open('all_sentences.txt', 'r', encoding='utf-8') as f:\n",
    "    all_sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.759 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['希拉里 克林顿 的 亚洲 之 行 ： 放心 吧 ， 盟友', '<sos> Secretary Clinton s Asia Trip Allied Reassurance', 'Secretary Clinton s Asia Trip Allied Reassurance <eos>']\n",
      "['进入 奥巴马 内阁 仅仅 三周 ， 国务卿 希拉里 克林顿 的 亚洲 之 行 信号 多于 实质 。', '<sos> Coming only three weeks into the Obama Administration Secretary of State Hillary Clinton s Asia trip will be long on signals and short on substance', 'Coming only three weeks into the Obama Administration Secretary of State Hillary Clinton s Asia trip will be long on signals and short on substance <eos>']\n",
      "['这 并 不是 坏事 ， 特别 是 访问 向 美国 的 盟友 日本 和 韩国 发出 了 几个 极为重要 的 信息 。', '<sos> That is not necessarily a bad thing especially when it sends several critically important messages to allies Japan and South Korea', 'That is not necessarily a bad thing especially when it sends several critically important messages to allies Japan and South Korea <eos>']\n",
      "['她 的 访问 告诉 大家 亚洲 事务 对于 美国 来说 很 重要 ， 华盛顿 致力于 长期 在 这个 区域 内 扮演 一个 有 影响力 的 角色 。', '<sos> Her trip communicates that Asia matters to the United States and that Washington is committed to a predominant role in the region over the long term', 'Her trip communicates that Asia matters to the United States and that Washington is committed to a predominant role in the region over the long term <eos>']\n",
      "['在 访问 北京 之前 访问 东京 和 首尔 反映 出 了 我们 的 盟友 的 重要性 以及 想要 缓解 “ 日本 已经 过气 了 ” 的 担心 的 直接 尝试 。', '<sos> Traveling to Tokyo and Seoul prior to Beijing reflects the importance of our allies as well as a direct attempt to assuage fears of Japan passing', 'Traveling to Tokyo and Seoul prior to Beijing reflects the importance of our allies as well as a direct attempt to assuage fears of Japan passing <eos>']\n"
     ]
    }
   ],
   "source": [
    "# 创建用于存储中英文句子对的列表\n",
    "sentences = []\n",
    "\n",
    "# 对每个句子进行处理\n",
    "for i in range(0, len(all_sentences), 2):\n",
    "    # 对中文句子进行分词\n",
    "    sentence_cn = ' '.join(jieba.cut(all_sentences[i].strip(), cut_all=False))\n",
    "    # 对英文句子进行分词\n",
    "    sentence_en = ' '.join(re.findall(r'\\b\\w+\\b', all_sentences[i+1].strip()))\n",
    "    # 构建句子对，分别添加<sos>和<eos>标记\n",
    "    sentences.append([sentence_cn, '<sos> ' + sentence_en, sentence_en + ' <eos>'])\n",
    "\n",
    "# 前5个句子对的显示\n",
    "for s in sentences[:5]:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: 构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子数量： 43509\n",
      "中文词汇表大小： 59930\n",
      "英文词汇表大小： 45152\n",
      "中文词汇到索引的字典： [('<pad>', 0), ('竭尽全力', 1), ('印度', 2), ('踢掉', 3), ('寝室', 4), ('攻城略地', 5), ('咸肉', 6), ('后半部', 7), ('美国市场', 8), ('玛法', 9), ('调料', 10), ('Ms', 11), ('堡', 12), ('新政府', 13), ('增高', 14), ('朵', 15), ('08', 16), ('苦学', 17), ('贸易法', 18), ('骑士', 19)]\n",
      "英文词汇到索引的字典： [('<pad>', 0), ('<sos>', 16610), ('<eos>', 43062), ('mile', 3), ('relationship', 4), ('complaining', 5), ('prepped', 6), ('constructive', 7), ('810', 8), ('Salt', 9), ('Ms', 10), ('enteric', 11), ('marvelled', 12), ('offshoots', 13), ('Asset', 14), ('interviews', 15), ('08', 16), ('Tranquility', 17), ('eyeliner', 18), ('inSeptember', 19)]\n"
     ]
    }
   ],
   "source": [
    "word_list_cn, word_list_en = [], []  # 初始化中英文单词列表\n",
    "# 遍历每一个句子并将单词添加到单词列表中\n",
    "for s in sentences:\n",
    "    word_list_cn.extend(s[0].split())\n",
    "    word_list_en.extend(s[1].split())\n",
    "    word_list_en.extend(s[2].split())\n",
    "# 去重得到不重复的单词列表\n",
    "word_list_cn = list(set(word_list_cn))\n",
    "word_list_en = list(set(word_list_en))\n",
    "\n",
    "# Add special tokens to the vocabulary\n",
    "word_list_cn = ['<pad>'] + word_list_cn\n",
    "word_list_en = ['<pad>', '<sos>', '<eos>'] + word_list_en\n",
    "\n",
    "# 构建单词到索引的映射\n",
    "word2idx_cn = {w: i for i, w in enumerate(word_list_cn)}\n",
    "word2idx_en = {w: i for i, w in enumerate(word_list_en)}\n",
    "\n",
    "# 构建索引到单词的映射\n",
    "idx2word_cn = {i: w for i, w in enumerate(word_list_cn)}\n",
    "idx2word_en = {i: w for i, w in enumerate(word_list_en)}\n",
    "\n",
    "# 计算词汇表的大小\n",
    "voc_size_cn = len(word_list_cn)\n",
    "voc_size_en = len(word_list_en)\n",
    "\n",
    "print(\"句子数量：\", len(sentences)) # 打印句子数\n",
    "print(\"中文词汇表大小：\", voc_size_cn) #打印中文词汇表大小\n",
    "print(\"英文词汇表大小：\", voc_size_en) #打印英文词汇表大小\n",
    "print(\"中文词汇到索引的字典：\", list(word2idx_cn.items())[:20]) # 中文词汇到索引\n",
    "print(\"英文词汇到索引的字典：\", list(word2idx_en.items())[:20]) # 英文词汇到索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, sentences, word2idx_cn, word2idx_en):\n",
    "        self.sentences = sentences\n",
    "        self.word2idx_cn = word2idx_cn\n",
    "        self.word2idx_en = word2idx_en\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 将句子转换为索引\n",
    "        sentence_cn = [self.word2idx_cn[word] for word in self.sentences[index][0].split()]\n",
    "        sentence_en_in = [self.word2idx_en[word] for word in self.sentences[index][1].split()]\n",
    "        sentence_en_out = [self.word2idx_en[word] for word in self.sentences[index][2].split()]\n",
    "        return torch.tensor(sentence_cn), torch.tensor(sentence_en_in), torch.tensor(sentence_en_out)\n",
    "\n",
    "# Collate function to pad sentences in a batch\n",
    "def collate_fn(batch):\n",
    "    # Sort the batch by the length of the sentences in descending order\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sentence_cn, sentence_en_in, sentence_en_out = zip(*batch)\n",
    "    # Pad the sentences\n",
    "    sentence_cn = nn.utils.rnn.pad_sequence(sentence_cn, padding_value=word2idx_cn['<pad>'])\n",
    "    sentence_en_in = nn.utils.rnn.pad_sequence(sentence_en_in, padding_value=word2idx_en['<sos>'])\n",
    "    sentence_en_out = nn.utils.rnn.pad_sequence(sentence_en_out, padding_value=word2idx_en['<eos>'])\n",
    "    return sentence_cn, sentence_en_in, sentence_en_out\n",
    "\n",
    "# 创建数据集\n",
    "dataset = TranslationDataset(sentences, word2idx_cn, word2idx_en)\n",
    "# 创建数据加载器，pass collate_fn to DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "      \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.fc(output.squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(2, batch_size, self.hidden_size)\n",
    "    \n",
    "# Check if a GPU is available and if not, use a CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "model = Seq2Seq(voc_size_cn, 256, voc_size_en)\n",
    "# Move the model to the device (GPU if available, otherwise CPU)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bleu_score\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_bleu\u001b[39m(model, dataloader):\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "from torchtext.data.metrics import bleu_score\n",
    "def compute_bleu(model, dataloader):\n",
    "    model.eval()\n",
    "    total_score = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sentence_cn, sentence_en_in, sentence_en_out in dataloader:\n",
    "            sentence_cn = sentence_cn.to(device)\n",
    "            sentence_en_in = sentence_en_in.to(device)\n",
    "            sentence_en_out = sentence_en_out.to(device)\n",
    "            \n",
    "            hidden = model.init_hidden(sentence_cn.size(1)).to(device)\n",
    "            output, hidden = model(sentence_en_in, hidden)\n",
    "            \n",
    "            # Convert output to predicted tokens\n",
    "            pred_tokens = output.argmax(2).detach().cpu().numpy().tolist()\n",
    "            target_tokens = sentence_en_out.cpu().numpy().tolist()\n",
    "\n",
    "            pred_sentences = [[str(token) for token in sentence] for sentence in pred_tokens]\n",
    "            target_sentences = [[[str(token) for token in sentence]] for sentence in target_tokens]\n",
    "\n",
    "            # Calculate BLEU score\n",
    "            for pred_sentence, target_sentence in zip(pred_sentences, target_sentences):\n",
    "                total_score += bleu_score([pred_sentence], [target_sentence])\n",
    "\n",
    "    return total_score / len(dataloader)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc='Epoch {:03d}'.format(epoch + 1), leave=False, disable=False)\n",
    "    for sentence_cn, sentence_en_in, sentence_en_out in progress_bar:\n",
    "        sentence_cn = sentence_cn.to(device)\n",
    "        sentence_en_in = sentence_en_in.to(device)\n",
    "        sentence_en_out = sentence_en_out.to(device)\n",
    "        \n",
    "        hidden = model.init_hidden(sentence_cn.size(1)).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(sentence_en_in, hidden)\n",
    "        loss = criterion(output.view(-1, voc_size_en), sentence_en_out.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(sentence_cn))})\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {total_loss/len(dataloader)}\")\n",
    "    bleu = compute_bleu(model, dataloader)\n",
    "    print(f\"Bleu Score: {bleu}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
